## PURPOSE
This technical documentation presents the guidelines (e.g server installation instructions, configuration of system elements) allowing preparation, configuration and maintenant in operational state of the development servers.

The DEV cluster is dedicated for run of a version of CYBNITY software components which is not already released (e.g staging version, feature branch version) and that is in current development by CYBNITY developers.

The services provided by the DEV cluster are focus on:
- unique resources server providing K8S resources (storage, memory, processing units) sharing between developers as centralized usabel instance for software suite unstable revisions deployment
- detection of sub-resources sizing requirements generated by any change of CYBNITY applications
- unit test and/or integration development activities (e.g link to other external systems that integration development is in progress)

# HARDWARE LAYER
Current hardware configuration is based on server:
- server type: Hewlett-Packard Z640
- CPU: 2 x Intel Xeon E5-2673 v4, 20 cores
- RAM: 256 GB
- Hard disks:
  - NVMe SSD 512 GB (Operation System & Linux based applications)
  - 1 SATA Disk 1 TB (K8S applications data)
- NVIDIA Quadro P4000 graphic card, 8Go
- 1 NIC 1Gbps: used for Wake-On-Lan (remote start of server)
- 2 x NIC 10Gbps:
  - 1 dedicated ot Kubernetes HA clustering
  - 1 not used
  
## BIOS & operating system layer
See [Ubuntu-installation](CYDEL01-ubuntu-installation.md) procedure to prepare a server into a __"ready for virtualization installation"__ state.

Current prepared server configuration is:
- hostname: __dev__
- FQDN: __dev.cybnity.tech__

# VIRTUALIZATION LAYER
RKE2 virtualization system is implemented as Kubernetes layer deployed onto __Dev cluster__ servers.

## Docker
- Docker engine shall be installed on server as documented on [Docker for Ubuntu documentation](https://docs.docker.com/engine/install/ubuntu/).
- Execute Linux post-installation steps for Docker Engine as [documented](https://docs.docker.com/engine/install/linux-postinstall/) allowing Docker usage as a non-root user.
- Add default DEV user account into the docker group, and/or member roles referenced into in the DEV Cluster configuration under Rancher.
- Configure Docker to start on boot with systemd

## Server ports
Open traffic on server allowing any discussion between __dev.cybnity.tech__ server and SUPPORT cluster servers like documented on [RKE doc](https://rke.docs.rancher.com/os#ports).

## RKE2 Kubernetes distribution
### DNS Resolver
By default, external server ip address and hostnames is not visible from pods deployed into the cluster.

Like RKE2 is by default reading the resolv.conf of Linux layer, add DNS servers definition as Global setting:
```
  # Read resolution status
  sudo resolvectl status

  # Update Global defaut DNS servers definition
  sudo vi /etc/systemd/resolved.conf

  # Uncomment property of [Resolve] section in file with
  DNS=192.168.60.28 192.168.60.1

  # restart resolver
  sudo systemctl restart systemd-resolved

  # Check updated Global DNS servers declaration
  sudo resolvectl status
```

### Networking issue resolution
According to the K8S network management system implemented by the DEV cluster, create the configuration files documented by [RKE2 know issues resolution means](https://docs.rke2.io/known_issues).

### Agent installation
RKE2 agent node is automatically installed on the server via DEV Cluster managed by Rancher (SUPPORT cluster).

From DEV Cluster server, execute (in `sudo` mode) the URL provided by Rancher regarding the dynamic Cluster created (e.g dev-deploy cluster created via Rancher UI).
The executed installation script manages the deployment of all RKE2 components required for runtime.

- Check the started rke2-agent service via commands:
```
  systemctl status rancher-system-agent

  # show logs in real-time
  journalctl -u rancher-system-agent -f

  # check pods status
  sudo /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get pods -A

  # --- When crash of cattle-system-agent ---
  # Apply patch to resolve Cattle Cluster Agent DNS issue with host alias

  sudo /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml -n cattle-system patch deployments cattle-cluster-agent --patch '{"spec": {"template": {"spec": {"hostAliases": [{"hostnames":["rancher.cybnity.tech"],"ip": "192.168.30.2"}]}}}}'
```

# INFRASTRUCTURE SERVICES

## Storage

## Monitoring & Logging

## Networking
By default, NetworkManager (create [configuration file](https://docs.rke2.io/known_issues) at __/etc/NetworkManager/NetworkManager.conf__) is started by Ubuntu (and managing dynamic resolv.conf update) and status can be checked via commands:
```
  sudo systemctl status systemd-resolved

  # stop and rke2-server
  sudo systemctl stop rke2-server.service
  sudo systemctl start rke2-server.service
```

### External FQDN visibility
By default, pods deployed into the cluster can't reach external server based on DNS (e.g Internet server name; external network server based on a FQDN and/or dns hostname).

When existing DNS server allowing external access to a cluster server node via hostname.domain_name (e.g sup1.cybnity.tech,, sup2.cybnity.tech, sup3.cybnity.tech), the pods deployed into the cluster can find any other external resource from  their FQDNs.

When none external DNS server is managing the server hostname and domain identification, a CoreDNS configuration file can be created allowing visibility of external machines (e.g Support cluster machine from the DEV cluster isolated network), and extending the default coredns config file automatically created by the Support server during the RKE2 dynamic agent installation.

## Security

## Cluster Nodes Availability Plan
Automatic poweroff and restart of cluster node can be managed via custom scheduling planified (for 24/24 availability security, when multiple DEV cluster nodes are existing a minimum of 2 nodes shall be always be in active state to ensure etc database sync).

### 8/24 hr - 2/7 days Availability Stop Plan
- __Servers Scheduling Stop Plan__ (controlled by Linux crontab service)

|Period|Task Time|Server Node|Comment            |Residual Accepted Risk|
|:-----|:--------|:----------|:-- ---------------|:---------------------|
|Daily |20:00    |dev        |None active cluster|Interrupted services  |

#### On each DEV cluster server
Defined crontab directives on each server in a safe way (with wait of existing process secure end before make the stop) via power off:
- Open and add command into crontab via command: `sudo crontab -e`
- Add line in file and save:
```
# Stop and poweroff server in a secure way (shutdown -P) each day at 20:00:00
0 20 * * * shutdown -P
```
- Crontab plan checking via command: `sudo crontab -l`

### 8/24 hr - 2/7 days Availability Start Plan
Controlled by BIOS setup, or via crontab on permanent available server (e.g dev.cybnity.tech).

- __Servers Scheduling Start Plan__

|Period            |Task Time|Server Node     |Comment                         |Residual Accepted Risk                |
|:-----------------|:--------|:---------------|:-------------------------------|:-------------------------------------|
|Thursday, Friday  |09:00    |dev             |Ordered WOL from HA proxy server|Cluster unique instance unavailability|
|Manual Wake-On-Lan|         |dev             |                                |Cluster unique instance unavailability|

#### On server managing start plan (HA server)
Define WOL script executing WOL call (e.g by ha.cybnity.tech server) via `/usr/local/bin/CYDEL_dev_cluster_start.sh` executable script) and crontab orchestration. See [CYDEL01-HA documentation](CYDEL01-HA.md) for more detail.

## Cluster remote management
On Rancher:
- Create a new cluster (e.g named DEV) into a Rancher application (executed on SUPPORT cluster) from Cluster Management section
- In Member Roles section, add user authorized to cluster management

On cluster eligible to remote management:
- Set the cluster-admin privileges for Rancher user via the command proposed by Rancher Registration section
- From cluster to control, execute the registration command proposed by Rancher (e.g via kubectl on existing DEV cluster) to import it into Rancher management system

## Kubernetes tools usage
When a K8S client is used, the automatic installed RKE2 CLI is hosted in __/var/lib/rancher/rke2/bin__ folder, and the kubeconfig location (automatically installed in __/etc/rancher/rke2__ folder) shall be identified during any CLI command execution.

For example:
```
# Show started nodes
/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get nodes

# Show deployed pods
/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get nodes -A

# Show errors relative to pods instantiation
/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get pods -v=10
```

# APPLICATION SERVICES

#
[Back To Home](CYDEL01.md)
