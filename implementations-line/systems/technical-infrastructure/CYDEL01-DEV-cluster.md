## PURPOSE
This technical documentation presents the guidelines (e.g server installation instructions, configuration of system elements) allowing preparation, configuration and maintenant in operational state of the development servers.

The DEV cluster is dedicated for run of a version of CYBNITY software components which is not already released (e.g staging version, feature branch version) and that is in current development by CYBNITY developers.

The services provided by the DEV cluster are focus on:
- unique resources server providing K8S resources (storage, memory, processing units) sharing between developers as centralized usabel instance for software suite unstable revisions deployment
- detection of sub-resources sizing requirements generated by any change of CYBNITY applications
- unit test and/or integration development activities (e.g link to other external systems that integration development is in progress)

# HARDWARE LAYER
Current hardware configuration is based on server:
- server type: Hewlett-Packard Z640
- CPU: 2 x Intel Xeon E5-2673 v4, 80 cores
- RAM: 256 GB
- Hard disks:
  - NVMe SSD 512 GB (Operation System & Linux based applications)
  - 1 SATA Disk 1 TB (K8S applications data)
- NVIDIA Quadro P4000 graphic card, 8Go
- 1 NIC 1Gbps: used for Wake-On-Lan (remote start of server)
- 2 x NIC 10Gbps:
  - 1 dedicated ot Kubernetes HA clustering
  - 1 not used
  
## BIOS & operating system layer
See [Ubuntu-installation](CYDEL01-ubuntu-installation.md) procedure to prepare a server into a __ready for virtualization installation__ state.

Current prepared server configuration is:
- hostname: __dev__
- FQDN: __dev.cybnity.tech__

# VIRTUALIZATION LAYER
RKE2 virtualization system is implemented as Kubernetes layer deployed onto __Dev cluster__ servers.

## Server ports
Open traffic on server allowing any discussion between __dev.cybnity.tech__ server and SUPPORT cluster servers like documented on [RKE doc](https://rke.docs.rancher.com/os#ports).

## RKE2 Node installation
### DNS Resolver
By default, external server ip address and hostnames is not visible from pods deployed into the cluster.

Like RKE2 is by default reading the resolv.conf of Linux layer, add DNS servers definition as Global setting:
```
  # Read resolution status
  sudo resolvectl status

  # Update Global defaut DNS servers definition
  sudo vi /etc/systemd/resolved.conf

  # Uncomment property of [Resolve] section in file with
  DNS=192.168.60.28 192.168.60.1

  # restart resolver
  sudo systemctl restart systemd-resolved

  # Check updated Global DNS servers declaration
  sudo resolvectl status
```

### Networking issue resolution
According to the K8S network management system implemented by the DEV cluster, create the configuration files documented by [RKE2 know issues resolution means](https://docs.rke2.io/known_issues):

- Creation of `90-rke2.conf` file into `/etc/sysctl.d` including:
  ```
    # Customization of settings defined by default 99-sysctl.conf file

    # Uncomment the next line to enable packet forwarding for IPv4
    net.ipv4.ip_forward=1

    # Custom RKE2 Wicked configuration for RKE2
    net.ipv4.conf.all.forwarding=1

    # Uncomment the next line to enable packet forwarding for IPv6
    #  Enabling this option disables Stateless Address Autoconfiguration
    #  based on Router Advertisements for this host
    net.ipv6.conf.all.forwarding=1
  ```
- Reload systemctl directives via command: ```sudo service procps force-reload```

### DEV cluster preparation
When DEV cluster is not already existing for receive new RKE2 node, create it from Rancher UI web console (clusters management tool accessible via web browser relative to SUPPORT cluster's Rancher application):
- From Rancher Clusters management view (e.g [CYBNITY CYDEL01 Rancher view](https://rancher.cybnity.tech/dashboard/home)):
  - Start creation of a new cluster (via __CREATE__ button)
  - Check that RKE2/K3s feature is active, and select `Custom` template for RKE2/K3s cluster type
- From opened view relative to new Cluster creation, set all information defining the new cluster configuration:
<details>
  <summary>Basics</summary>

  |Property|Value|Comments|
  |:-------|:----|:-------|
  |Cluster Name|dev-deploy|e.g `dev-deploy` equals to isolated context for cluster targeting to host CYBNITY software suite revisions in development phase; see CYBNITY branching model for help|
  |Cloud Provider|Default-RKE2 Embedded| |
  |Container Network|calico| |
  |Security CIS Profile|(None)| |
  |Pod Security Admission Configuration Template|Default-RKE2 Embedded| |
  |Project Network Isolation|(None)| |
  |System Services|CoreDNS + NGINX Ingress + Metrics Server|All services active for each cluster node|
</details>

<details>
  <summary>Member Roles</summary>

  |Property|Value|Comments|
  |:-------|:----|:-------|
  |User|Local|Add each user account having responsibility on the cluster management|
</details>

<details>
  <summary>Cluster Agent</summary>

  |Property|Value|Comments|
  |:-------|:----|:-------|
  |Affinity|Use default affinity rules defined by Rancher| |
</details>

<details>
  <summary>etcd</summary>

  |Property|Value|Comments|
  |:-------|:----|:-------|
  |Automatic Snapshots|Enable|Default cron scheduling (0 */2 * * *) and keeped last (5)|
  |Backup Snapshots to S3|Disable|Possible change for enabling to MinIO cluster if accessigne from context|
  |Metrics|Exposed to the public interface| |
</details>

<details>
  <summary>Fleet Agent</summary>

  |Property|Value|Comments|
  |:-------|:----|:-------|
  |Affinity|Use default affinity rules defined by Rancher| |
</details>

<details>
  <summary>Networking</summary>

  #### Addressing
  |Property|Value|Comments|
  |:-------|:----|:-------|
  |Cluster Domain|cybnity.tech|Visibility between services in cluster with any FQDN externally managed over the HA service|

  #### TLS Alternate Names
  |Property|Value|Comments|
  |:-------|:----|:-------|
  |wildcard name|*.cybnity.tech| |

  #### Authorized Endpoint
  Enabled.
  |Property|Value|Comments|
  |:-------|:----|:-------|
  |FQDN|dev-deploy.cybnity.tech|Allow cluster access over external HA server, and access to other *.cybnity.tech server (e.g Rancher)|
  |CA Certificates|star-cybnity-tech-full-validation-chain.pem text|Include all certificates involved into the validation of keys/certificates on cybnity.tech network zone, and/or exposure of CYBNITY SOftware Suite endpoints over an HTTPS protocol with CA validation|
</details>

<details>
  <summary>Upgrade Strategy</summary>

  |Property|Value|Comments|
  |:-------|:----|:-------|
  |Control Plane Concurrency|1| |
  |Worker Concurrency|1| |

  #### Drain Nodes (Control Plane)
  Yes.
  |Property|Value|Comments|
  |:-------|:----|:-------|
  |Delete pods using emptyDir volumes|true| |
  |Delete standalone pods|false| |
  |Override pod termination grace periods|false| |
  |Timeout after|120|Default value|

  #### Drain Nodes (Worker Nodes)
  Yes.
  |Property|Value|Comments|
  |:-------|:----|:-------|
  |Delete pods using emptyDir volumes|true| |
  |Delete standalone pods|false| |
  |Override pod termination grace periods|false| |
  |Timeout after|120|Default value|
</details>

### Agent installation
RKE2 agent node is automatically installed on the server via DEV Cluster managed by Rancher (SUPPORT cluster).

From DEV Cluster server, execute (in `sudo` mode) the URL provided by Rancher regarding the dynamic Cluster created (e.g dev-deploy cluster created via Rancher UI).
The executed installation script manages the deployment of all RKE2 components required for runtime.

- Check the started rke2-agent service via commands:
```
  sudo systemctl status rancher-system-agent

  # show logs in real-time
  sudo journalctl -u rancher-system-agent -f

  # check pods status
  sudo /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get pods -A

  # --- When crash of cattle-system-agent ---
  # Apply patch to resolve Cattle Cluster Agent DNS issue with host alias

  sudo /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml -n cattle-system patch deployments cattle-cluster-agent --patch '{"spec": {"template": {"spec": {"hostAliases": [{"hostnames":["rancher.cybnity.tech"],"ip": "192.168.30.2"}]}}}}'
```

### Kubernetes CLI
For simplify usage of Kubernetes tools automatically installed on the node:
- Create symbolic links to RKE2 tools (kubectl, kubelet, crictl, ctr) via commands:
```
  sudo ln -s /var/lib/rancher/rke2/bin/kubectl /usr/local/bin/kubectl
  sudo ln -s /var/lib/rancher/rke2/bin/kubelet /usr/local/bin/kubelet
  sudo ln -s /var/lib/rancher/rke2/bin/crictl /usr/local/bin/crictl
  sudo ln -s /var/lib/rancher/rke2/bin/ctr /usr/local/bin/ctr
```

### DEV Cluster configuration files
For simplify usage of RKE2 cluster configuration automatically installed on the node (RKE2 default __cluster configuration file is rke2.yaml__):
- Create symbolic link to the created kubeconfig file via command:
```
  mkdir -p ~/.kube
  sudo ln -s /etc/rancher/rke2/rke2.yaml ~/.kube/config
```

### Cluster remote management
From DEV cluster node eligible to remote management, get a copy of the `/etc/rancher/rke2/rke2.yaml` file allowing cluster management from external administration tool (e.g LENS standalone application for Kubernetes clusters management).

Change of included server hostname value (e.g originally valued as `server: https://127.0.0.1:6443`) to externally visible cluster node hostname (e.g `server: https://dev.cybnity.tech:6443`).

## RKE2 Node cleanup
RKE2 node cleanup to reset a cluster node, run the following commands:
```
  # rke2-(server|agent) related
  sudo rke2-killall.sh
  sudo rke2-uninstall.sh
  # rancher-system-agent related
  sudo systemctl stop rancher-system-agent.service
  sudo systemctl disable rancher-system-agent.service
  sudo rm -f /etc/systemd/system/rancher-system-agent.service
  sudo rm -f /etc/systemd/system/rancher-system-agent.env
  sudo systemctl daemon-reload
  sudo rm -f /usr/local/bin/rancher-system-agent
  sudo rm -rf /etc/rancher/
  sudo rm -rf /var/lib/rancher/
  sudo rm -rf /usr/local/bin/rke2*
```

# INFRASTRUCTURE SERVICES

## Storage

## Monitoring & Logging

## Networking
By default, NetworkManager (create [configuration file](https://docs.rke2.io/known_issues) at __/etc/NetworkManager/NetworkManager.conf__) is started by Ubuntu (and managing dynamic resolv.conf update) and status can be checked via commands:
```
  sudo systemctl status systemd-resolved

  # stop and rke2-server
  sudo systemctl stop rke2-server.service
  sudo systemctl start rke2-server.service
```

### External FQDN visibility
By default, pods deployed into the cluster can't reach external server based on DNS (e.g Internet server name; external network server based on a FQDN and/or dns hostname).

When existing DNS server allowing external access to a cluster server node via hostname.domain_name (e.g sup1.cybnity.tech,, sup2.cybnity.tech, sup3.cybnity.tech), the pods deployed into the cluster can find any other external resource from  their FQDNs.

When none external DNS server is managing the server hostname and domain identification, a CoreDNS configuration file can be created allowing visibility of external machines (e.g Support cluster machine from the DEV cluster isolated network), and extending the default coredns config file automatically created by the Support server during the RKE2 dynamic agent installation.

## Security

## Cluster Nodes Availability Plan
Automatic poweroff and restart of cluster node can be managed via custom scheduling planified (for 24/24 availability security, when multiple DEV cluster nodes are existing a minimum of 2 nodes shall be always be in active state to ensure etc database sync).

### 8/24 hr - 2/7 days Availability Stop Plan
- __Servers Scheduling Stop Plan__ (controlled by Linux crontab service)

|Period|Task Time|Server Node|Comment            |Residual Accepted Risk|
|:-----|:--------|:----------|:------------------|:---------------------|
|Daily |20:00    |dev        |None active cluster|Interrupted services  |

#### On each DEV cluster server
Defined crontab directives on each server in a safe way (with wait of existing process secure end before make the stop) via power off:
- Open and add command into crontab via command: `sudo crontab -e`
- Add line in file and save:
```
# Stop and poweroff server in a secure way (shutdown -P) each day at 20:00:00
0 20 * * * shutdown -P
```
- Crontab plan checking via command: `sudo crontab -l`

### 8/24 hr - 2/7 days Availability Start Plan
Controlled by BIOS setup, or via crontab on permanent available server (e.g dev.cybnity.tech).

- __Servers Scheduling Start Plan__

|Period            |Task Time|Server Node     |Comment                         |Residual Accepted Risk                |
|:-----------------|:--------|:---------------|:-------------------------------|:-------------------------------------|
|Thursday, Friday  |09:00    |dev             |Ordered WOL from HA proxy server|Cluster unique instance unavailability|
|Manual Wake-On-Lan|         |dev             |                                |Cluster unique instance unavailability|

#### On server managing start plan (HA server)
Define WOL script executing WOL call (e.g by ha.cybnity.tech server) via `/usr/local/bin/CYDEL_dev_cluster_start.sh` executable script) and crontab orchestration. See [CYDEL01-HA documentation](CYDEL01-HA.md) for more detail.

## Kubernetes tools usage
When a K8S client is used, the automatic installed RKE2 CLI is hosted in __/var/lib/rancher/rke2/bin__ folder, and the kubeconfig location (automatically installed in __/etc/rancher/rke2__ folder) shall be identified during any CLI command execution.

For example:
```
  # Show started nodes
  sudo kubectl --kubeconfig=.kube/config get nodes

  # Show deployed pods
  sudo kubectl --kubeconfig=.kube/config get nodes -A

  # Show errors relative to pods instantiation
  sudo kubectl --kubeconfig=.kube/config get pods -v=10
```

# APPLICATION SERVICES

## DEV Node labelling
CYBNITY Software suite revision deployment is based on a dissimination of application services through a logical model of application layers.

Pre-established node labels are used to identify where each application module shall be started during a CYBNITY software suite full deployment on a cluster.

According to the quantity of DEV cluster nodes provisioned, the labels shall be assigned to nodes reserved as __CYBNITY application architecture zones__.

### Unique cluster node
In case of DEV cluster based on unique node, all the labels shall be assigned to the same node.
- Copy script file `add-labels-to-dev-cluster.sh` ([script version](../modules/dev-env/cluster/kubectl/add-labels-to-dev-cluster.sh) maintained in __systems/modules/dev-env/cluster/kubectl__ folder and assigning the labels to an unique started node) and save it into `/usr/local/bin` folder
- Make each script executable via command: `sudo chmod +x /usr/local/bin/*.sh`

#
[Back To Home](CYDEL01.md)
